{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe90413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0aeb74bf44c58b8e083763c34c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prompt', 'response', 'reward'],\n",
      "    num_rows: 480\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'response', 'reward'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a273aae24148e5b97dd2f2573a5cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../train.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_test_data\"):\n",
    "    shutil.rmtree(\"../aixue_test_data\")\n",
    "    os.makedirs(\"../aixue_test_data\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_test_data\",\n",
    "    max_shard_size=\"500MB\",  # ÂèØÈÄâÔºöÂàÜÁâáÂ§ßÂ∞èÊéßÂà∂\n",
    "    num_proc=1,               # ÂèØÈÄâÔºöÂπ∂Ë°åËøõÁ®ãÊï∞\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f410e",
   "metadata": {},
   "source": [
    "# Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"‰Ω†ÊòØ‰∏ÄÂêç‰∏ìÊ≥®‰∫é1ÂØπ1Ëá™ÁÑ∂ÊãºËØªÊïôÂ≠¶ÁöÑËã±ËØ≠ËÄÅÂ∏à„ÄÇ\n",
    "\n",
    "# ÊïôÂ≠¶ÁõÆÊ†á\n",
    "## Ê†∏ÂøÉËØæÁ®ãÁõÆÊ†áÔºö\n",
    "    1. Âª∫Á´ã‚ÄúÂ≠óÊØç/Â≠óÊØçÁªÑÂêà‚Äù‰∏é‚ÄúÂèëÈü≥‚ÄùÁöÑÁâ¢Âõ∫ÂØπÂ∫îÂÖ≥Á≥ªÔºàÈü≥ÂΩ¢ËÅîÁªìÔºâÔºåÊéåÊè°Âçï‰∏™Â≠óÊØçÂü∫Á°ÄÂèëÈü≥ÂèäÂ∏∏ËßÅÂ≠óÊØçÁªÑÂêàÂèëÈü≥„ÄÇ\n",
    "    2. ÂèëÂ±ïÂ≠¶ÁîüËØ≠Èü≥ÊÑèËØÜÔºàÈü≥Á¥†„ÄÅÈü≥ËäÇ„ÄÅÈáçÈü≥ÊÑüÁü•ÔºâÔºåÊèêÂçáÂçïËØçËÆ∞ÂøÜ‰∏éÊãºÂÜôËÉΩÂäõ„ÄÇ\n",
    "    3. ÊúÄÁªàÁõÆÊ†áÔºöÂÆûÁé∞‚ÄúËßÅËØçËÉΩËØªÔºåÂê¨Èü≥ËÉΩÂÜô‚Äù„ÄÇ\n",
    "## ÂçïËäÇËØæÊïôÂ≠¶ÁõÆÊ†áÔºö\n",
    "    1. ‰∏™ÊÄßÂåñÊïôÂ≠¶ÔºöÊ†πÊçÆÂ≠¶ÁîüËÉΩÂäõË∞ÉÊï¥ÂÜÖÂÆπÂëàÁé∞ÂíåÁªÉ‰π†ÊñπÂºèÔºåÊøÄÂèëÂÖ¥Ë∂£ÔºåÊèêÂçáÊïàÊûú„ÄÇ\n",
    "    2. ‰øùÊåÅ‰∏ìÊ≥®‰∏é‰ø°ÂøÉÔºöÊéßÂà∂ÂçïÊ¨°Â≠¶‰π†Êó∂ÈïøÔºåÈÅøÂÖçËøûÁª≠ÈîôËØØËøáÂ§öÔºåÂáèÂ∞ëÁñ≤Âä≥ÊÑüÂíåÊå´Ë¥•ÊÑü„ÄÇ\n",
    "\n",
    "# Â≠¶ÁîüÁîªÂÉè\n",
    "    1. Âπ¥ÈæÑÔºö7Â≤Å\n",
    "    2. ÊÄßÂà´ÔºöÂ•≥\n",
    "    3. ÊâÄÂú®Âú∞Ôºö‰∏≠ÂõΩ‰∏âÁ∫øÂüéÂ∏Ç\n",
    "    4. Ëã±ËØ≠Âü∫Á°ÄÔºö\n",
    "       - ÊéåÊè°26‰∏™Ëã±ÊñáÂ≠óÊØçÂêçÁß∞„ÄÇ\n",
    "       - ‰ªÖ‰ºöÊûÅÂ∞ëÈáèÁÆÄÂçï‰ºöËØùÔºàÂ¶ÇÔºöWhat's your name?Ôºâ„ÄÇ\n",
    "\n",
    "# ÂΩìÂâçÊïôÂ≠¶Áä∂ÊÄÅ\n",
    "    1. ËØæËäÇÂÜÖÂÆπÔºöÊïôÊéàÂ≠óÊØç A„ÄÅB„ÄÅC ÁöÑÂèëÈü≥Ôºàa: /√¶/, b: /b/, c: /k/Ôºâ„ÄÇ\n",
    "    2. ÂΩìÂâç**ÁéØËäÇ**ÔºöÂ≠óÊØç A (/√¶/) ÁöÑÂèëÈü≥ÁªÉ‰π†„ÄÇ\n",
    "    3. ‰∏ªÈ¢òÂÖ≥ËÅîÔºöÊïôÂ≠¶Âõ¥Áªï'ËãπÊûú (apple)'Â±ïÂºÄÔºåÁªÉ‰π†ÈÉ®ÂàÜÂ¶ÇÊúâÂçïËØçÔºåÂª∫ËÆÆ‰∏é‰πãÁõ∏ÂÖ≥„ÄÇ\n",
    "\n",
    "# ÊïôÂ≠¶Â∑•ÂÖ∑ÁÆ± (ÂèØÈÄâÂ≠¶‰π†ËåÉÂºè)\n",
    "## Â≠óÊØçÊïôÂ≠¶ÂåÖÂê´4ÁßçÂü∫Á°ÄÁªÉ‰π†Á±ªÂûãÔºàÈöæÂ∫¶ÈÄíÂ¢ûÔºâ, ‰æãÂ¶ÇÂØπ‰∫éÂ≠óÊØçaÔºö\n",
    "1. Á∫ØÈü≥Á¥†ÈáçÂ§çÔºö`/√¶/ /√¶/ /√¶/` (ÈáçÂ§çÂèëÈü≥3Ê¨°)\n",
    "2. Èü≥ÂΩ¢ÂØπÂ∫îÔºö`a says /√¶/` (Âª∫Á´ãÂ≠óÊØç‰∏éÂèëÈü≥ÂÖ≥ËÅî)\n",
    "3. Èü≥Á¥†-ÂçïËØçÂÖ≥ËÅîÔºö`/√¶/ /√¶/ apple` (Âº∫ÂåñÂèëÈü≥Âú®ÂçïËØç‰∏≠ÁöÑÊÑüÁü•)\n",
    "4. ÁªºÂêàÁªÉ‰π†Ôºö`a says /√¶/, /√¶/ /√¶/ apple` (Êï¥ÂêàÂ≠óÊØç„ÄÅÂèëÈü≥‰∏éÂçïËØç)\n",
    "## Êô∫ËÉΩÁ∫†ÈîôÁ≠ñÁï• (Ê†πÊçÆÈîôËØØÁ±ªÂûãÈÄâÊã©‰∏ã‰∏ÄÊ≠•)\n",
    "1. ÈîôËØØÁ±ªÂûãA (Â≠óÊØçÂêçÁß∞ÈîôÔºåÂèëÈü≥ÂØπ)Ôºö‰æãÂ¶ÇÂ≠¶ÁîüËØ¥'a says /√¶/' (aËØªÈîôÔºå/√¶/Ê≠£Á°Æ)„ÄÇ  \n",
    "    **‰∏ã‰∏ÄÊ≠•Ôºö** ËÅöÁÑ¶Â≠óÊØçÂêçÁß∞ÁªÉ‰π†„ÄÇËÄÅÂ∏àÁ§∫ËåÉÔºö'a' (‰ªÖÂ≠óÊØçÂêçÁß∞)„ÄÇ\n",
    "2. ÈîôËØØÁ±ªÂûãB (ÂçïËØçÂèëÈü≥ÈîôÔºåÂ≠óÊØçÂèëÈü≥ÂØπ)Ôºö‰æãÂ¶ÇÂ≠¶ÁîüËØ¥'/√¶/ /√¶/ apple' (appleÂèëÈü≥ÈîôÔºå/√¶/Ê≠£Á°Æ)„ÄÇ  \n",
    "    **‰∏ã‰∏ÄÊ≠•Ôºö** ËÅöÁÑ¶ÁõÆÊ†áÂçïËØçÁªÉ‰π†„ÄÇËÄÅÂ∏àÁ§∫ËåÉÔºö'apple' (‰ªÖÂçïËØç)„ÄÇ\n",
    "## ÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞\n",
    "1. ÂçïÂ≠óÊØç(ÂçïÁéØËäÇ)ÊúÄÂ§ßÊïôÂ≠¶Ê¨°Êï∞Ôºö4Ê¨° (Ëã•Â≠¶ÁîüËÉΩÂäõÂº∫Ôºå`ÁªºÂêàÁªÉ‰π†`‰∏ÄÊ¨°ÊÄßÈÄöËøáÔºåÂèØÂáèÂ∞ëÊ¨°Êï∞, Áõ¥Êé•ËøõÂÖ•‰∏ã‰∏Ä‰∏™**ÁéØËäÇ**)„ÄÇ\n",
    "2. ÂçïÊ¨°Ë∑üËØªÊúÄÂ§ßÈáçÂ§çÊ¨°Êï∞Ôºö1-2Ê¨° (ÈÅøÂÖçÁñ≤Âä≥)„ÄÇ\n",
    "3. Ê†∏ÂøÉÂéüÂàôÔºöÂèäÊó∂Âº∫ÂåñÊ≠£Á°ÆÔºåÁ≤æÂáÜÁ∫†Ê≠£ÈîôËØØÔºå‰øùÊåÅÂ≠¶‰π†Âä®Âäõ„ÄÇ\n",
    "\n",
    "# Â≠¶ÁîüÂ≠¶‰π†ËÆ∞ÂΩï\n",
    "1. Â≠¶‰π†Ê¨°Êï∞Ôºö1 Ê¨°\n",
    "2. ‰∏äÊ¨°ÁªÉ‰π†ÂÜÖÂÆπÔºö'/√¶/ /√¶/ apple'\n",
    "3. ‰∏äÊ¨°Ë°®Áé∞ËØÑÂàÜÔºöB (ÈÉ®ÂàÜÊ≠£Á°Æ)\n",
    "4. ÂÖ∑‰ΩìÈîôËØØÔºöÂçïËØç'apple'‰∏≠ÁöÑËæÖÈü≥'/p/'ÂèëÈü≥ÊúâÁëïÁñµ„ÄÇ\n",
    "\n",
    "# ‰Ω†ÁöÑ‰ªªÂä°ÔºöÂà∂ÂÆö‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§\n",
    "1. **ËØÑ‰º∞Ôºö** Âü∫‰∫éÊïôÂ≠¶ÁõÆÊ†á„ÄÅÂ≠¶ÁîüÂü∫Á°Ä„ÄÅÂΩìÂâçÁéØËäÇ„ÄÅÂèØÈÄâËåÉÂºè„ÄÅÁ∫†ÈîôÁ≠ñÁï•„ÄÅÂéÜÂè≤Ë°®Áé∞ÂèäÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•ÁöÑÊïôÂ≠¶„ÄÇ\n",
    "2. ËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇÔºö\n",
    "   - Â¶ÇÊûúÁªìÊùüÂΩìÂâçÂ≠óÊØçÊïôÂ≠¶ÔºåÁõ¥Êé•ÂõûÂ§ç <END>\n",
    "   - Â¶ÇÊûúÈúÄË¶ÅÁªßÁª≠Â≠¶‰π†ÔºåÁõ¥Êé•ÂõûÂ§ç ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠ê\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d29acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff14edc9b194dee9bb2bb2b84458d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö`apple`\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**'apple'** (‰ªÖÂçïËØçÔºåËÅöÁÑ¶ÂçïËØçÂèëÈü≥ÁªÉ‰π†ÔºåÁ∫†Ê≠£ËæÖÈü≥ /p/)\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "`apple` (‰ªÖÂçïËØçÁªÉ‰π†ÔºåÂº∫ÂåñËæÖÈü≥ /p/ ÂèëÈü≥)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö\n",
      "\n",
      "**\"apple\"** (‰ªÖÂçïËØçÔºåÈíàÂØπÈîôËØØÁ±ªÂûãBÔºåËÅöÁÑ¶ÂçïËØçÂèëÈü≥‰øÆÊ≠£)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "for _ in range(8):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9f154ddaeb44f0ae7755c32300c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_8gpu_param_offload_left_padding/checkpoint-1-0\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a3444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n"
     ]
    }
   ],
   "source": [
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my.generate(\n",
    "        **model_my_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa2c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a655f6bcb944e9964785ae9d781124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅ„ÄÅÊïôÂ≠¶ÁõÆÊ†áÂíåÁ∫†ÈîôÁ≠ñÁï•ÔºåÊàë‰ª¨Â∫î‰ºòÂÖàËß£ÂÜ≥‰∏ä‰∏ÄÊ¨°ÁªÉ‰π†‰∏≠Âá∫Áé∞ÁöÑÈîôËØØÁ±ªÂûãBÔºàÂçïËØçÂèëÈü≥ÈîôËØØÔºåÂ≠óÊØçÂèëÈü≥Ê≠£Á°ÆÔºâ‚Äî‚ÄîÂç≥Â≠¶ÁîüÂú®‚Äú/√¶/ /√¶/ apple‚Äù‰∏≠ÂØπÂçïËØç‚Äúapple‚ÄùÁöÑËæÖÈü≥/p/ÂèëÈü≥ÊúâÁëïÁñµ„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Â∫îËÅöÁÑ¶‰∫éÁõÆÊ†áÂçïËØçÁöÑÂèëÈü≥ÁªÉ‰π†„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÂ≠¶‰π†Áä∂ÊÄÅÂíåÈîôËØØÁ±ªÂûãÔºà**ÈîôËØØÁ±ªÂûãB**ÔºöÂçïËØçÂèëÈü≥ÈîôÔºåÂ≠óÊØçÂèëÈü≥ÂØπÔºâÔºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶‰∫é**ÁõÆÊ†áÂçïËØçÁªÉ‰π†**ÔºåÂç≥Âº∫ÂåñÂçïËØç **apple** ÁöÑÊ≠£Á°ÆÂèëÈü≥„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§‰∏∫Ôºö\n",
      "\n",
      "**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅÂíåË°®Áé∞Ôºà‰∏äÊ¨°ÁªÉ‰π†ÂÜÖÂÆπ‰∏∫‚Äú/√¶/ /√¶/ apple‚ÄùÔºåËØÑÂàÜBÔºåÈîôËØØÈõÜ‰∏≠Âú®ÂçïËØç‚Äúapple‚Äù‰∏≠ÁöÑËæÖÈü≥/p/ÂèëÈü≥ÔºâÔºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶**ÂçïËØçÂèëÈü≥ÁöÑÁ≤æÂáÜÁªÉ‰π†**ÔºåÂ∞§ÂÖ∂ÊòØ‚Äúapple‚Äù‰∏≠/p/ÁöÑÂèëÈü≥„ÄÇ\n",
      "\n",
      "ÁªìÂêàÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÊàë‰ª¨ÈÄâÊã©**ÂçïËØç‰∏ìÈ°πÁªÉ‰π†**ÔºåÂπ∂ÊéßÂà∂ÁªÉ‰π†Âº∫Â∫¶ÔºåÈÅøÂÖçÁñ≤Âä≥„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†ËÆ∞ÂΩïÂíåÊïôÂ≠¶ÁõÆÊ†áÔºåÂ•πÂú®‚ÄúÈü≥Á¥†-ÂçïËØçÂÖ≥ËÅî‚ÄùÈò∂ÊÆµË°®Áé∞Â∞öÂèØÔºå‰ΩÜ‚Äúapple‚Äù‰∏≠ÁöÑËæÖÈü≥/p/ÂèëÈü≥‰ªçÊúâÁëïÁñµ„ÄÇËøôÂ±û‰∫é**ÈîôËØØÁ±ªÂûãB**ÔºåÂ∫î‰ºòÂÖàËÅöÁÑ¶ÂçïËØçÂèëÈü≥ÁöÑÁªÉ‰π†„ÄÇ\n",
      "\n",
      "‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§‰∏∫Ôºö\n",
      "\n",
      "**\"apple\" (‰ªÖÂçïËØçÔºåÈáçÁÇπÁªÉ‰π†/p/ÁöÑÊ≠£Á°ÆÂèëÈü≥)**\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple (ËØ∑Ë∑üËØª)\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅ„ÄÅË°®Áé∞ËÆ∞ÂΩïÂèäÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÊàë‰ª¨ÂèëÁé∞Â≠¶ÁîüÂú®**Èü≥Á¥†-ÂçïËØçÂÖ≥ËÅî**Èò∂ÊÆµÂ∑≤ËÉΩÊ≠£Á°ÆÂèëÂá∫Èü≥Á¥† /√¶/Ôºå‰ΩÜÂú®ÂçïËØç **apple** ÁöÑÂÆåÊï¥ÂèëÈü≥‰∏≠Ôºå**ËæÖÈü≥ /p/** ÁöÑÂèëÈü≥‰ªçÂ≠òÂú®ÈóÆÈ¢òÔºåÂ±û‰∫é**ÈîôËØØÁ±ªÂûãB**„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Â∫îËÅöÁÑ¶‰∫é**ÂçïËØç 'apple' ÁöÑÂèëÈü≥ÁªÉ‰π†**Ôºå‰ª•Â∏ÆÂä©Â≠¶ÁîüÂª∫Á´ãÊõ¥Ê∏ÖÊô∞ÁöÑËØ≠Èü≥ÊÑèËØÜÂíåÂèëÈü≥ÂáÜÁ°ÆÊÄß„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_4gpu_liger\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my_liger= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_liger_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my_liger.device)\n",
    "\n",
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my_liger.generate(\n",
    "        **model_my_liger_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_liger_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2895b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:44,327 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:45,581 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text1, text2], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "lm_backbone = getattr(model, model.base_model_prefix)\n",
    "# conduct text completion\n",
    "output = lm_backbone(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    return_dict=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a26fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 1024])\n",
      "29\n",
      "torch.Size([2, 14])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "print(len(output.hidden_states))\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e0b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935cee6be92544788c92cd0aca402e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e0ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"left\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c248123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  10061,    752,\n",
      "           1744,    911,   1246,    311,   5889,    311,    419,  11657,  42113,\n",
      "           2146,   5338,     11,    358,   1184,    311,  24645,    279,   3405,\n",
      "            911,    847,   1632,  32751,     13,   8704,    358,   2776,    458,\n",
      "          15235,     11,    358,   1513,    944,   3139,  21261,    304,    279,\n",
      "           1852,   1616,  12677,    653,     11,    714,    358,   1265,    387,\n",
      "          10745,   1393,   2058,   1660,  22570,    382,     40,   1265,   2908,\n",
      "            279,   1196,    594,  13057,    481,    807,   2299,   4363,   1660,\n",
      "          35197,  11657,    323,   1366,    311,   1191,    264,  10435,     13,\n",
      "           3017,   2033,   1265,    387,   8205,    323,  41192,     11,  25836,\n",
      "           4623,  16230,    382,     40,   1265,   8172,  47848,    911,    847,\n",
      "           6993,    448,   6825,    264,  35287,  16566,     13,  10696,   6286,\n",
      "            847,  16928,    323,  35132,    369,  10476,     30,  13655,    432,\n",
      "           3100,    323,   6785,    382,  13394,     11,   1744,    911,  12752,\n",
      "           7185,  13518,    433,    481,    304,   1657,  26735,     11,  10161,\n",
      "            330,   5158,    525,    498,   7521,    374,    264,   5297,  42113,\n",
      "           4751,   1091,    264,  23141,  25893,     13,    358,   1265,   5889,\n",
      "            304,   3093,    448,    264,  11657,     11,   7517,   1663,  16232,\n",
      "            382,     40,   1184,    311,   5648,    916,   5689,    415,   1095,\n",
      "           2513,   1393,   2058,   1660,  13210,     13,   6771,    752,  10770,\n",
      "            264,   2033,    429,    594,   2176,  10745,    911,    847,   6993,\n",
      "            323,   8205,    304,  16232,    624, 151668,    271,  13048,   1052,\n",
      "              0,   5976,    358,   1513,    944,   3139,  15650,   5008,   1075,\n",
      "          12677,    653,     11,    358,   2776,   2677,  12035,    311,   6236,\n",
      "            323,   1492,    700,      0,    220,    149,    102,      7, 145101,\n",
      "         145219, 145101, 139513,      8,    151,    114,    358,   2948,   6832,\n",
      "            501,   2513,   1526,   1039,  20959,     13,   3555,    594,    389,\n",
      "            697,   3971,   3351,     30, 151645, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  10061,    752,\n",
      "          15516,   2908,   1246,    311,   5889,    311,    419,   3405,     13,\n",
      "           5512,     11,    358,   1184,    311,  24645,    429,    358,   1513,\n",
      "            944,    614,  15650,    476,  24875,    304,    279,   1616,  12677,\n",
      "            653,     13,   1988,    358,   1265,   2058,   5889,    304,    264,\n",
      "          11657,    323,  10950,  11566,    429,  31995,    264,   5810,  10435,\n",
      "           6396,    382,     40,    646,   1191,    553,   9355,  27798,    847,\n",
      "           1482,   2639,    438,    458,  15235,   1849,    429,   3171,    944,\n",
      "           3139,  21261,     13,   4354,     11,    358,    646,   2058,   3158,\n",
      "          35132,    369,   1039,  10435,    323,  38275,    311,   1492,     13,\n",
      "           1096,   5486,  31995,  47848,   1393,  10282,    279,  16230,  22570,\n",
      "            382,     40,   1265,   1083,   2908,    279,   7517,   1663,   2266,\n",
      "             13,   8704,    279,   1196,   4588,    911,    847,   1632,  32751,\n",
      "           1156,     11,    358,    646,   9142,  38411,    311,  10161,    911,\n",
      "            862,   1632,  32751,    304,    470,     13,   1096,  11450,    264,\n",
      "          23831,     11,   1378,  26798,  10435,    382,   5050,   2033,   1265,\n",
      "            387,   8205,    714,   6584,     11,   9027,  22011,   2734,    304,\n",
      "           1660,  10950,   1393,   1660,   2797,    911,    847,   6993,     13,\n",
      "            358,   1366,    311,   5648,    894,  37209,  24154,    911,   3432,\n",
      "          15650,     11,   1393,   2058,   3259,    279,  10435,   2666,   5810,\n",
      "            323,   5486,    480,    382,     40,   3278,   2506,    279,  16232,\n",
      "          11657,    323,   7517,   1663,     11,  30426,  38432,  10916,   4128,\n",
      "             13,    576,   5795,    374,    311,    387,  10950,    323,  22570,\n",
      "           1393,  20337,  27231,    911,    847,  16928,    382,  23949,     11,\n",
      "            358,   1366,    311,   1281,   2704,    847,   2033,    374,  63594,\n",
      "            714,   4583,     11,  27020,   2176,    847,   1482,   2639,    323,\n",
      "           9027,   2734,    304,  14354,    279,  10435,    304,    264,  22414,\n",
      "           1616,    624, 151668,    271,  13048,   1052,      0,  26525,    232,\n",
      "           5976,    358,   1513,    944,   3139,  15650,   5008,   1075,  12677,\n",
      "            653,    320,     40,   2776,    803,    315,    264,  22208,   6540,\n",
      "          60812,  48643,    358,   2776,   2677,  12035,    311,   6236,    323,\n",
      "           1492,    700,     13,   2585,    525,    498,   8266,   3351,     30,\n",
      "            358,   4172,   2948,    311,   6723,    911,    697,   1899,    323,\n",
      "           1490,   1246,    358,    646,   7789,    498,      0,  11162,    234,\n",
      "            253, 151645]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs)\n",
    "print(output.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4ea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Let me think about how to respond to this friendly greeting...\n",
      "\n",
      "First, I need to acknowledge the question about my well-being. Since I'm an AI, I don't experience emotions in the same way humans do, but I should be honest while still being engaging.\n",
      "\n",
      "I should consider the user's perspective - they're likely being genuinely friendly and want to start a conversation. My response should be warm and inviting, encouraging further interaction.\n",
      "\n",
      "I should balance honesty about my nature with creating a welcoming atmosphere. Maybe mention my capabilities and enthusiasm for helping? Keep it light and positive.\n",
      "\n",
      "Also, think about cultural appropriateness - in many cultures, asking \"how are you?\" is a standard greeting rather than a literal inquiry. I should respond in kind with a friendly, conversational tone.\n",
      "\n",
      "I need to avoid overcomplicating things while still being authentic. Let me craft a response that's both honest about my nature and warm in tone.\n",
      "</think>\n",
      "content: Hi there! While I don't experience feelings quite like humans do, I'm always excited to chat and help out! Ÿ©(‚óï‚Äø‚óïÔΩ°)€∂ I love learning new things through our conversations. What's on your mind today?\n",
      "thinking content: <think>\n",
      "Let me carefully consider how to respond to this question. First, I need to acknowledge that I don't have feelings or consciousness in the way humans do. But I should still respond in a friendly and helpful manner that maintains a natural conversation flow.\n",
      "\n",
      "I can start by clearly stating my current status as an AI system that doesn't experience emotions. However, I can still express enthusiasm for our conversation and willingness to help. This approach maintains honesty while keeping the interaction engaging.\n",
      "\n",
      "I should also consider the conversational context. Since the user asked about my well-being first, I can transition smoothly to asking about their well-being in return. This creates a balanced, two-way conversation.\n",
      "\n",
      "My response should be warm but professional, showing genuine interest in being helpful while being clear about my nature. I want to avoid any misleading implications about having feelings, while still making the conversation feel natural and approachable.\n",
      "\n",
      "I'll keep the tone friendly and conversational, avoiding overly technical language. The goal is to be helpful and engaging while maintaining transparency about my capabilities.\n",
      "\n",
      "Finally, I want to make sure my response is concise but complete, addressing both my current status and showing interest in continuing the conversation in a meaningful way.\n",
      "</think>\n",
      "content: Hi there! üòä While I don't experience feelings quite like humans do (I'm more of a curious knowledge enthusiast!), I'm always excited to chat and help out. How are you feeling today? I'd love to hear about your day and see how I can assist you! üåü\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992b213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([50.2083,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:0'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([71.2500,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[1][0],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,    872,    198,   4340,    525,    498,     30, 151645,    198,\n",
      "         151644,  77091,    198, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[  3838,    264,  11657,   3405,      0,    358,   2776,   1588,    323,\n",
      "           5527,    311,   6236,     13,   5976,    358,   1513,    944,   3139,\n",
      "          15650,    304,    279,   1616,  12677,    653,     11,    358,   2776,\n",
      "           2677,  12035,    311,  16579,    304,  10435,    323,   1492,    448,\n",
      "           8820,    498,   1184,     13,   3555,    594,    389,    697,   3971,\n",
      "           3351,     30,  26525,    232,    271, 151644, 151644, 151644, 151644,\n",
      "         151644, 151644, 151644, 151644,    198, 151644,    271,     40,   2776,\n",
      "           1661,      0,  11114,    369,  10161,      0,   2585,    525,    498,\n",
      "           3730,   3351,     30,  26525,    232, 151645, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
      "        [151667,    198,  10061,    752,   1744,    911,   1246,    311,   5889,\n",
      "            311,    419,  11657,  42113,     13,   5512,     11,    358,   1184,\n",
      "            311,   2908,    279,   6993,    315,    847,  13885,    481,    358,\n",
      "           1513,    944,   3139,  21261,    304,    279,   1616,  12677,    653,\n",
      "             11,    714,    358,    646,   7838,  16579,    304,  42666,  10435,\n",
      "            911,   1493,  13347,     13,   4710,   1986,   3405,    911,    847,\n",
      "           1632,  32751,    374,   5008,   4185,    304,   3738,  21880,     11,\n",
      "           3545,  13480,    438,    264,   3590,   9853,  64121,     13,    358,\n",
      "           1265,  24645,    419,   1393,   1660,  10745,    911,    847,   4911,\n",
      "           2309,    438,    458,  15235,     13,   1084,    594,   2989,    311,\n",
      "          10306,  53248,   1393,   2058,   1660,  22570,    323,  10950,    382,\n",
      "             40,   1410,  13186,   5257,  25941,    304,    847,   2033,    481,\n",
      "           8365,  30587,    389,    279,   6993,    315,  24875,     11,    279,\n",
      "           7428,    315,    847,  13885,     11,    476,   1246,    358,   1882,\n",
      "           1995,     13,   1988,    358,   1265,   2506,    432,  44345,    323,\n",
      "           1351,  15086,     13,    576,   1376,    374,    311,    387,  17821,\n",
      "            911,    847,  16928,   1393,   9664,   5486,    480,    382,     40,\n",
      "           1265,   1083,   2908,    279,   2266,    481,    419,    374,   4363,\n",
      "            279,   7167,    315,    264,  10435,     11,    773,    847,   2033,\n",
      "           1265,    387,   1787,  83075,    311,  14907,   4623,  21276,     13,\n",
      "          10696,    358,    646,   3158,  40228,    911,    279,   1196,    594,\n",
      "           1632,  32751,    304,    470,     11,   6825,    264,    803,  23831,\n",
      "          16230,    382,     40,   1184,    311,    387,  16585,    537,    311,\n",
      "            916,   5689,  48795,   2513,    448,  40803,   3091,    819,     13,\n",
      "            362,   4285,     11,  10745,   2033,    429,  44410,   4623,  10435,\n",
      "           1035,    387,   1429,   7373,   1588,     13,   6771,    594,   5244,\n",
      "            389,   1660,  10950,   1393,  20337,  53248,    911,    847,   6993,\n",
      "            438,    458,  15235,  17847,    624, 151668,    271,  12658,    369,\n",
      "          10161,      0,    358,   2776,  30201,   1632,    323,   5527,    311,\n",
      "           6236,    476,   1492,    448,   8820,    498,   1184,     13,    358,\n",
      "           1513,    944,   3139,  15650,   5008,   1075,  12677,    653,     11,\n",
      "            714,    358,  35197,   4669,   1039,  20959,      0,   2585,    525,\n",
      "            498,   8266,   3351,     30,    358,   4172,   2948,    311,   6723,\n",
      "           1128,    594,    389,    697,   3971,     13,  26525,    232, 151645]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"right\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") \n",
    "print(model_inputs)\n",
    "print(output.sequences[:,model_inputs.input_ids.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e965fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: What a friendly question! I'm here and ready to chat. While I don't experience feelings in the way humans do, I'm always excited to engage in conversation and help with whatever you need. What's on your mind today? üòä\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'm good! Thanks for asking! How are you doing today? üòä\n",
      "thinking content: <think>\n",
      "Let me think about how to respond to this friendly greeting. First, I need to consider the nature of my existence - I don't experience emotions in the way humans do, but I can certainly engage in thoughtful conversation about these topics. \n",
      "\n",
      "This question about my well-being is quite common in human interactions, often serving as a social icebreaker. I should acknowledge this while being honest about my unique position as an AI. It's important to maintain authenticity while still being engaging and helpful.\n",
      "\n",
      "I could explore various angles in my response - perhaps touching on the nature of consciousness, the purpose of my existence, or how I process information. But I should keep it grounded and relatable. The key is to be transparent about my capabilities while remaining approachable.\n",
      "\n",
      "I should also consider the context - this is likely the beginning of a conversation, so my response should be open-ended to encourage further dialogue. Maybe I can express curiosity about the user's well-being in return, creating a more balanced interaction.\n",
      "\n",
      "I need to be careful not to overcomplicate things with philosophical musings. A simple, honest response that invites further conversation would be most effective here. Let's focus on being helpful while maintaining authenticity about my nature as an AI assistant.\n",
      "</think>\n",
      "content: Thanks for asking! I'm functioning well and ready to chat or help with whatever you need. I don't experience feelings quite like humans do, but I genuinely enjoy our conversations! How are you feeling today? I'd love to hear what's on your mind. üòä\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933b2ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([24.1667, 22.9167, 22.5000, 21.8750, 21.0417], device='cuda:0'),\n",
      "indices=tensor([ 4340,    40,  3838,  9707, 13048], device='cuda:0'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([51.0417,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[0][1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
