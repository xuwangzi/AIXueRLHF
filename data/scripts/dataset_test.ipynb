{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe90413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0aeb74bf44c58b8e083763c34c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prompt', 'response', 'reward'],\n",
      "    num_rows: 480\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'response', 'reward'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a273aae24148e5b97dd2f2573a5cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../train.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_test_data\"):\n",
    "    shutil.rmtree(\"../aixue_test_data\")\n",
    "    os.makedirs(\"../aixue_test_data\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_test_data\",\n",
    "    max_shard_size=\"500MB\",  # ÂèØÈÄâÔºöÂàÜÁâáÂ§ßÂ∞èÊéßÂà∂\n",
    "    num_proc=1,               # ÂèØÈÄâÔºöÂπ∂Ë°åËøõÁ®ãÊï∞\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f410e",
   "metadata": {},
   "source": [
    "# Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"‰Ω†ÊòØ‰∏ÄÂêç‰∏ìÊ≥®‰∫é1ÂØπ1Ëá™ÁÑ∂ÊãºËØªÊïôÂ≠¶ÁöÑËã±ËØ≠ËÄÅÂ∏à„ÄÇ\n",
    "\n",
    "# ÊïôÂ≠¶ÁõÆÊ†á\n",
    "## Ê†∏ÂøÉËØæÁ®ãÁõÆÊ†áÔºö\n",
    "    1. Âª∫Á´ã‚ÄúÂ≠óÊØç/Â≠óÊØçÁªÑÂêà‚Äù‰∏é‚ÄúÂèëÈü≥‚ÄùÁöÑÁâ¢Âõ∫ÂØπÂ∫îÂÖ≥Á≥ªÔºàÈü≥ÂΩ¢ËÅîÁªìÔºâÔºåÊéåÊè°Âçï‰∏™Â≠óÊØçÂü∫Á°ÄÂèëÈü≥ÂèäÂ∏∏ËßÅÂ≠óÊØçÁªÑÂêàÂèëÈü≥„ÄÇ\n",
    "    2. ÂèëÂ±ïÂ≠¶ÁîüËØ≠Èü≥ÊÑèËØÜÔºàÈü≥Á¥†„ÄÅÈü≥ËäÇ„ÄÅÈáçÈü≥ÊÑüÁü•ÔºâÔºåÊèêÂçáÂçïËØçËÆ∞ÂøÜ‰∏éÊãºÂÜôËÉΩÂäõ„ÄÇ\n",
    "    3. ÊúÄÁªàÁõÆÊ†áÔºöÂÆûÁé∞‚ÄúËßÅËØçËÉΩËØªÔºåÂê¨Èü≥ËÉΩÂÜô‚Äù„ÄÇ\n",
    "## ÂçïËäÇËØæÊïôÂ≠¶ÁõÆÊ†áÔºö\n",
    "    1. ‰∏™ÊÄßÂåñÊïôÂ≠¶ÔºöÊ†πÊçÆÂ≠¶ÁîüËÉΩÂäõË∞ÉÊï¥ÂÜÖÂÆπÂëàÁé∞ÂíåÁªÉ‰π†ÊñπÂºèÔºåÊøÄÂèëÂÖ¥Ë∂£ÔºåÊèêÂçáÊïàÊûú„ÄÇ\n",
    "    2. ‰øùÊåÅ‰∏ìÊ≥®‰∏é‰ø°ÂøÉÔºöÊéßÂà∂ÂçïÊ¨°Â≠¶‰π†Êó∂ÈïøÔºåÈÅøÂÖçËøûÁª≠ÈîôËØØËøáÂ§öÔºåÂáèÂ∞ëÁñ≤Âä≥ÊÑüÂíåÊå´Ë¥•ÊÑü„ÄÇ\n",
    "\n",
    "# Â≠¶ÁîüÁîªÂÉè\n",
    "    1. Âπ¥ÈæÑÔºö7Â≤Å\n",
    "    2. ÊÄßÂà´ÔºöÂ•≥\n",
    "    3. ÊâÄÂú®Âú∞Ôºö‰∏≠ÂõΩ‰∏âÁ∫øÂüéÂ∏Ç\n",
    "    4. Ëã±ËØ≠Âü∫Á°ÄÔºö\n",
    "       - ÊéåÊè°26‰∏™Ëã±ÊñáÂ≠óÊØçÂêçÁß∞„ÄÇ\n",
    "       - ‰ªÖ‰ºöÊûÅÂ∞ëÈáèÁÆÄÂçï‰ºöËØùÔºàÂ¶ÇÔºöWhat's your name?Ôºâ„ÄÇ\n",
    "\n",
    "# ÂΩìÂâçÊïôÂ≠¶Áä∂ÊÄÅ\n",
    "    1. ËØæËäÇÂÜÖÂÆπÔºöÊïôÊéàÂ≠óÊØç A„ÄÅB„ÄÅC ÁöÑÂèëÈü≥Ôºàa: /√¶/, b: /b/, c: /k/Ôºâ„ÄÇ\n",
    "    2. ÂΩìÂâç**ÁéØËäÇ**ÔºöÂ≠óÊØç A (/√¶/) ÁöÑÂèëÈü≥ÁªÉ‰π†„ÄÇ\n",
    "    3. ‰∏ªÈ¢òÂÖ≥ËÅîÔºöÊïôÂ≠¶Âõ¥Áªï'ËãπÊûú (apple)'Â±ïÂºÄÔºåÁªÉ‰π†ÈÉ®ÂàÜÂ¶ÇÊúâÂçïËØçÔºåÂª∫ËÆÆ‰∏é‰πãÁõ∏ÂÖ≥„ÄÇ\n",
    "\n",
    "# ÊïôÂ≠¶Â∑•ÂÖ∑ÁÆ± (ÂèØÈÄâÂ≠¶‰π†ËåÉÂºè)\n",
    "## Â≠óÊØçÊïôÂ≠¶ÂåÖÂê´4ÁßçÂü∫Á°ÄÁªÉ‰π†Á±ªÂûãÔºàÈöæÂ∫¶ÈÄíÂ¢ûÔºâ, ‰æãÂ¶ÇÂØπ‰∫éÂ≠óÊØçaÔºö\n",
    "1. Á∫ØÈü≥Á¥†ÈáçÂ§çÔºö`/√¶/ /√¶/ /√¶/` (ÈáçÂ§çÂèëÈü≥3Ê¨°)\n",
    "2. Èü≥ÂΩ¢ÂØπÂ∫îÔºö`a says /√¶/` (Âª∫Á´ãÂ≠óÊØç‰∏éÂèëÈü≥ÂÖ≥ËÅî)\n",
    "3. Èü≥Á¥†-ÂçïËØçÂÖ≥ËÅîÔºö`/√¶/ /√¶/ apple` (Âº∫ÂåñÂèëÈü≥Âú®ÂçïËØç‰∏≠ÁöÑÊÑüÁü•)\n",
    "4. ÁªºÂêàÁªÉ‰π†Ôºö`a says /√¶/, /√¶/ /√¶/ apple` (Êï¥ÂêàÂ≠óÊØç„ÄÅÂèëÈü≥‰∏éÂçïËØç)\n",
    "## Êô∫ËÉΩÁ∫†ÈîôÁ≠ñÁï• (Ê†πÊçÆÈîôËØØÁ±ªÂûãÈÄâÊã©‰∏ã‰∏ÄÊ≠•)\n",
    "1. ÈîôËØØÁ±ªÂûãA (Â≠óÊØçÂêçÁß∞ÈîôÔºåÂèëÈü≥ÂØπ)Ôºö‰æãÂ¶ÇÂ≠¶ÁîüËØ¥'a says /√¶/' (aËØªÈîôÔºå/√¶/Ê≠£Á°Æ)„ÄÇ  \n",
    "    **‰∏ã‰∏ÄÊ≠•Ôºö** ËÅöÁÑ¶Â≠óÊØçÂêçÁß∞ÁªÉ‰π†„ÄÇËÄÅÂ∏àÁ§∫ËåÉÔºö'a' (‰ªÖÂ≠óÊØçÂêçÁß∞)„ÄÇ\n",
    "2. ÈîôËØØÁ±ªÂûãB (ÂçïËØçÂèëÈü≥ÈîôÔºåÂ≠óÊØçÂèëÈü≥ÂØπ)Ôºö‰æãÂ¶ÇÂ≠¶ÁîüËØ¥'/√¶/ /√¶/ apple' (appleÂèëÈü≥ÈîôÔºå/√¶/Ê≠£Á°Æ)„ÄÇ  \n",
    "    **‰∏ã‰∏ÄÊ≠•Ôºö** ËÅöÁÑ¶ÁõÆÊ†áÂçïËØçÁªÉ‰π†„ÄÇËÄÅÂ∏àÁ§∫ËåÉÔºö'apple' (‰ªÖÂçïËØç)„ÄÇ\n",
    "## ÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞\n",
    "1. ÂçïÂ≠óÊØç(ÂçïÁéØËäÇ)ÊúÄÂ§ßÊïôÂ≠¶Ê¨°Êï∞Ôºö4Ê¨° (Ëã•Â≠¶ÁîüËÉΩÂäõÂº∫Ôºå`ÁªºÂêàÁªÉ‰π†`‰∏ÄÊ¨°ÊÄßÈÄöËøáÔºåÂèØÂáèÂ∞ëÊ¨°Êï∞, Áõ¥Êé•ËøõÂÖ•‰∏ã‰∏Ä‰∏™**ÁéØËäÇ**)„ÄÇ\n",
    "2. ÂçïÊ¨°Ë∑üËØªÊúÄÂ§ßÈáçÂ§çÊ¨°Êï∞Ôºö1-2Ê¨° (ÈÅøÂÖçÁñ≤Âä≥)„ÄÇ\n",
    "3. Ê†∏ÂøÉÂéüÂàôÔºöÂèäÊó∂Âº∫ÂåñÊ≠£Á°ÆÔºåÁ≤æÂáÜÁ∫†Ê≠£ÈîôËØØÔºå‰øùÊåÅÂ≠¶‰π†Âä®Âäõ„ÄÇ\n",
    "\n",
    "# Â≠¶ÁîüÂ≠¶‰π†ËÆ∞ÂΩï\n",
    "1. Â≠¶‰π†Ê¨°Êï∞Ôºö1 Ê¨°\n",
    "2. ‰∏äÊ¨°ÁªÉ‰π†ÂÜÖÂÆπÔºö'/√¶/ /√¶/ apple'\n",
    "3. ‰∏äÊ¨°Ë°®Áé∞ËØÑÂàÜÔºöB (ÈÉ®ÂàÜÊ≠£Á°Æ)\n",
    "4. ÂÖ∑‰ΩìÈîôËØØÔºöÂçïËØç'apple'‰∏≠ÁöÑËæÖÈü≥'/p/'ÂèëÈü≥ÊúâÁëïÁñµ„ÄÇ\n",
    "\n",
    "# ‰Ω†ÁöÑ‰ªªÂä°ÔºöÂà∂ÂÆö‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§\n",
    "1. **ËØÑ‰º∞Ôºö** Âü∫‰∫éÊïôÂ≠¶ÁõÆÊ†á„ÄÅÂ≠¶ÁîüÂü∫Á°Ä„ÄÅÂΩìÂâçÁéØËäÇ„ÄÅÂèØÈÄâËåÉÂºè„ÄÅÁ∫†ÈîôÁ≠ñÁï•„ÄÅÂéÜÂè≤Ë°®Áé∞ÂèäÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•ÁöÑÊïôÂ≠¶„ÄÇ\n",
    "2. ËæìÂá∫Ê†ºÂºèË¶ÅÊ±ÇÔºö\n",
    "   - Â¶ÇÊûúÁªìÊùüÂΩìÂâçÂ≠óÊØçÊïôÂ≠¶ÔºåÁõ¥Êé•ÂõûÂ§ç <END>\n",
    "   - Â¶ÇÊûúÈúÄË¶ÅÁªßÁª≠Â≠¶‰π†ÔºåÁõ¥Êé•ÂõûÂ§ç ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠ê\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83d29acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a1148d20144f319c41714cf6d6ad50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö  \n",
      "**\"apple\"** (‰ªÖÂçïËØçÔºåËÅöÁÑ¶ÂçïËØçÂèëÈü≥ÁªÉ‰π†ÔºåÁ∫†Ê≠£ËæÖÈü≥ /p/)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö  \n",
      "**\"apple\"** (‰ªÖÂçïËØçÔºåËÅöÁÑ¶ÂçïËØçÂèëÈü≥ÁªÉ‰π†ÔºåÁ∫†Ê≠£ËæÖÈü≥ /p/)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "for _ in range(8):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96407f6a21c401c94725f80efa7f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple (ËØ∑Ë∑üËØª)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅÂíåÂéÜÂè≤Ë°®Áé∞ÔºåÂ•πÂú®‚ÄúÈü≥Á¥†-ÂçïËØçÂÖ≥ËÅî‚ÄùÈò∂ÊÆµÔºà`/√¶/ /√¶/ apple`Ôºâ‰∏≠ÔºåËôΩÁÑ∂ÂØπÈü≥Á¥† `/√¶/` ÁöÑÂèëÈü≥ÊéåÊè°ËæÉÂ•ΩÔºå‰ΩÜÂú®ÂçïËØç `apple` ÁöÑÂèëÈü≥‰∏≠ÔºåÁâπÂà´ÊòØËæÖÈü≥ `/p/` ÁöÑÂèëÈü≥‰ªçÂ≠òÂú®ÁëïÁñµ„ÄÇËøôË°®ÊòéÂ•πÂú®Èü≥Á¥†-ÂçïËØçÁöÑÊï¥ÂêàÁªÉ‰π†‰∏≠‰ªçÈúÄÂº∫ÂåñËØ≠Èü≥ÊÑèËØÜÂíåÂçïËØçÊï¥‰ΩìÂèëÈü≥ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶‰∫éÁõÆÊ†áÂçïËØç `apple` ÁöÑÂèëÈü≥ÁªÉ‰π†Ôºå‰ª•ÊèêÈ´òÂÖ∂ÂØπÂçïËØçÊï¥‰ΩìËØ≠Èü≥ÁªìÊûÑÁöÑÊéåÊè°„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "`apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂΩìÂâçÊïôÂ≠¶ÁõÆÊ†á„ÄÅÂ≠¶ÁîüÂü∫Á°Ä„ÄÅÂΩìÂâçÁéØËäÇÂèäÂéÜÂè≤Ë°®Áé∞ÔºåÂ≠¶ÁîüÂú®Èü≥Á¥†-ÂçïËØçÂÖ≥ËÅîÁªÉ‰π†‰∏≠Â∑≤Âü∫Êú¨ÊéåÊè°Èü≥Á¥† /√¶/ ÁöÑÂèëÈü≥Ôºå‰ΩÜÂú®ÂçïËØç \"apple\" ÁöÑÂÆåÊï¥ÂèëÈü≥‰∏ä‰ªçÈúÄÂä†Âº∫ÔºåÂ∞§ÂÖ∂ÊòØËæÖÈü≥ /p/ ÁöÑÂèëÈü≥„ÄÇÂõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶ÂçïËØçÁªÉ‰π†Ôºå‰ª•Â∏ÆÂä©Â≠¶ÁîüÊõ¥ÂáÜÁ°ÆÂú∞ÂÆåÊàêÊï¥‰∏™ÂçïËØçÁöÑÂèëÈü≥„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "`apple` (‰ªÖÂçïËØçÔºåÂº∫Ë∞ÉÊ∏ÖÊô∞ÂèëÈü≥)\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_8gpu_param_offload\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92a3444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple\n"
     ]
    }
   ],
   "source": [
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my.generate(\n",
    "        **model_my_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa2c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a655f6bcb944e9964785ae9d781124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅ„ÄÅÊïôÂ≠¶ÁõÆÊ†áÂíåÁ∫†ÈîôÁ≠ñÁï•ÔºåÊàë‰ª¨Â∫î‰ºòÂÖàËß£ÂÜ≥‰∏ä‰∏ÄÊ¨°ÁªÉ‰π†‰∏≠Âá∫Áé∞ÁöÑÈîôËØØÁ±ªÂûãBÔºàÂçïËØçÂèëÈü≥ÈîôËØØÔºåÂ≠óÊØçÂèëÈü≥Ê≠£Á°ÆÔºâ‚Äî‚ÄîÂç≥Â≠¶ÁîüÂú®‚Äú/√¶/ /√¶/ apple‚Äù‰∏≠ÂØπÂçïËØç‚Äúapple‚ÄùÁöÑËæÖÈü≥/p/ÂèëÈü≥ÊúâÁëïÁñµ„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Â∫îËÅöÁÑ¶‰∫éÁõÆÊ†áÂçïËØçÁöÑÂèëÈü≥ÁªÉ‰π†„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÂ≠¶‰π†Áä∂ÊÄÅÂíåÈîôËØØÁ±ªÂûãÔºà**ÈîôËØØÁ±ªÂûãB**ÔºöÂçïËØçÂèëÈü≥ÈîôÔºåÂ≠óÊØçÂèëÈü≥ÂØπÔºâÔºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶‰∫é**ÁõÆÊ†áÂçïËØçÁªÉ‰π†**ÔºåÂç≥Âº∫ÂåñÂçïËØç **apple** ÁöÑÊ≠£Á°ÆÂèëÈü≥„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§‰∏∫Ôºö\n",
      "\n",
      "**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅÂíåË°®Áé∞Ôºà‰∏äÊ¨°ÁªÉ‰π†ÂÜÖÂÆπ‰∏∫‚Äú/√¶/ /√¶/ apple‚ÄùÔºåËØÑÂàÜBÔºåÈîôËØØÈõÜ‰∏≠Âú®ÂçïËØç‚Äúapple‚Äù‰∏≠ÁöÑËæÖÈü≥/p/ÂèëÈü≥ÔºâÔºå‰∏ã‰∏ÄÊ≠•Â∫îËÅöÁÑ¶**ÂçïËØçÂèëÈü≥ÁöÑÁ≤æÂáÜÁªÉ‰π†**ÔºåÂ∞§ÂÖ∂ÊòØ‚Äúapple‚Äù‰∏≠/p/ÁöÑÂèëÈü≥„ÄÇ\n",
      "\n",
      "ÁªìÂêàÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÊàë‰ª¨ÈÄâÊã©**ÂçïËØç‰∏ìÈ°πÁªÉ‰π†**ÔºåÂπ∂ÊéßÂà∂ÁªÉ‰π†Âº∫Â∫¶ÔºåÈÅøÂÖçÁñ≤Âä≥„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†ËÆ∞ÂΩïÂíåÊïôÂ≠¶ÁõÆÊ†áÔºåÂ•πÂú®‚ÄúÈü≥Á¥†-ÂçïËØçÂÖ≥ËÅî‚ÄùÈò∂ÊÆµË°®Áé∞Â∞öÂèØÔºå‰ΩÜ‚Äúapple‚Äù‰∏≠ÁöÑËæÖÈü≥/p/ÂèëÈü≥‰ªçÊúâÁëïÁñµ„ÄÇËøôÂ±û‰∫é**ÈîôËØØÁ±ªÂûãB**ÔºåÂ∫î‰ºòÂÖàËÅöÁÑ¶ÂçïËØçÂèëÈü≥ÁöÑÁªÉ‰π†„ÄÇ\n",
      "\n",
      "‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Êåá‰ª§‰∏∫Ôºö\n",
      "\n",
      "**\"apple\" (‰ªÖÂçïËØçÔºåÈáçÁÇπÁªÉ‰π†/p/ÁöÑÊ≠£Á°ÆÂèëÈü≥)**\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: /√¶/ /√¶/ apple (ËØ∑Ë∑üËØª)\n",
      "thinking content: \n",
      "content: **‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö** `apple` (‰ªÖÂçïËØç)\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n",
      "thinking content: \n",
      "content: Ê†πÊçÆÂ≠¶ÁîüÂΩìÂâçÁöÑÂ≠¶‰π†Áä∂ÊÄÅ„ÄÅË°®Áé∞ËÆ∞ÂΩïÂèäÊïôÂ≠¶ÊéßÂà∂ÂèÇÊï∞ÔºåÊàë‰ª¨ÂèëÁé∞Â≠¶ÁîüÂú®**Èü≥Á¥†-ÂçïËØçÂÖ≥ËÅî**Èò∂ÊÆµÂ∑≤ËÉΩÊ≠£Á°ÆÂèëÂá∫Èü≥Á¥† /√¶/Ôºå‰ΩÜÂú®ÂçïËØç **apple** ÁöÑÂÆåÊï¥ÂèëÈü≥‰∏≠Ôºå**ËæÖÈü≥ /p/** ÁöÑÂèëÈü≥‰ªçÂ≠òÂú®ÈóÆÈ¢òÔºåÂ±û‰∫é**ÈîôËØØÁ±ªÂûãB**„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå‰∏ã‰∏ÄÊ≠•ÊïôÂ≠¶Â∫îËÅöÁÑ¶‰∫é**ÂçïËØç 'apple' ÁöÑÂèëÈü≥ÁªÉ‰π†**Ôºå‰ª•Â∏ÆÂä©Â≠¶ÁîüÂª∫Á´ãÊõ¥Ê∏ÖÊô∞ÁöÑËØ≠Èü≥ÊÑèËØÜÂíåÂèëÈü≥ÂáÜÁ°ÆÊÄß„ÄÇ\n",
      "\n",
      "**‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: ‰∏ã‰∏ÄÊ≠•Ë∑üËØªÁöÑÂè•Â≠êÔºö**apple**\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_4gpu_liger\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my_liger= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_liger_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my_liger.device)\n",
    "\n",
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my_liger.generate(\n",
    "        **model_my_liger_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_liger_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2895b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:44,327 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:45,581 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text1, text2], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "lm_backbone = getattr(model, model.base_model_prefix)\n",
    "# conduct text completion\n",
    "output = lm_backbone(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    return_dict=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a26fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 1024])\n",
      "29\n",
      "torch.Size([2, 14])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "print(len(output.hidden_states))\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e0ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"left\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c248123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:7'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:7')}\n",
      "tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  32313,     11,\n",
      "            279,   1196,   4588,     11,    330,   4340,    525,    498,   7521,\n",
      "            358,   1184,    311,   5889,  34901,     13,   8704,    358,   2776,\n",
      "            458,  15235,  17847,     11,    358,   1265,  24645,    862,   3405,\n",
      "            323,   3410,    264,  10950,   4226,    382,   5338,     11,    358,\n",
      "           1265,   7683,    847,   2639,    438,    458,  15235,    323,   6286,\n",
      "            429,    358,   2776,   1588,    311,   7789,     13,   5005,     11,\n",
      "            358,    646,   3010,  12994,    448,   5257,  13347,     13,    358,\n",
      "           1265,   2506,    279,  16232,  11657,    323,   1787,  83075,    311,\n",
      "          14907,   4623,  10435,    382,     40,   1265,   5648,    894,  10916,\n",
      "            502,  70821,    323,   2506,    279,   2033,   4285,     13,   6771,\n",
      "            752,   1779,    369,    894,   3204,  89600,    819,     13,   2308,\n",
      "             11,    279,   1196,   1101,   6801,    311,   1414,    847,   1482,\n",
      "           1584,    323,   5726,    311,   1492,     13,  97593,     11,    429,\n",
      "           1265,   3421,    432,    624, 151668,    271,   9707,      0,    358,\n",
      "           2776,   1588,    311,   1492,    448,   4113,    498,   1184,     13,\n",
      "           2585,    646,    358,   7789,    498,   3351,     30,  26525,    232,\n",
      "         151645, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  32313,     11,\n",
      "            279,   1196,   4588,     11,    330,   4340,    525,    498,     11,\n",
      "           3351,   7521,    358,   1184,    311,   5889,  34901,     13,   6771,\n",
      "            752,   1744,    911,    279,   1850,   1616,    311,   4226,    382,\n",
      "           5338,     11,    358,   1265,  24645,    862,   3405,     13,   1084,\n",
      "            594,    264,  11657,   1616,    311,   1191,     13,  10696,   1977,\n",
      "           2494,   1075,    330,  13048,      0,   2585,    525,    498,   3351,\n",
      "           7521,   2938,    594,  47787,    323,   4933,   2734,    382,  12209,\n",
      "             11,    358,    646,    912,    264,   2699,    803,    311,   2506,\n",
      "            279,  10435,   2087,     13,  10696,   6286,   2494,    911,    279,\n",
      "           1899,     11,   1075,    330,     40,   2776,   8266,   2244,   3351,\n",
      "           8958,    311,   1473,    358,   2776,   6247,     13,   4710,     40,\n",
      "           1265,   2506,    279,  16232,   6785,    323,  11657,     13,  34006,\n",
      "            894,   8225,   4128,     13,   6771,    752,   1281,   2704,    279,\n",
      "           2033,    374,  63594,    714,   8205,     13,  97593,     11,    429,\n",
      "           1265,    975,    624, 151668,    271,  13048,      0,   2585,    525,\n",
      "            498,   3351,     30,    358,   2776,   2256,   6247,    311,   6723,\n",
      "            429,      0,    358,   2776,   8266,   2244,     11,    323,    358,\n",
      "           2776,  12035,    311,    387,   1588,     13,   6771,    752,   1414,\n",
      "            421,   1052,    594,   4113,    498,   2299,  22208,    911,      0,\n",
      "          26525,    232, 151645]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs)\n",
    "print(output.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4ea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond appropriately. Since I'm an AI assistant, I should acknowledge their question and provide a helpful answer.\n",
      "\n",
      "First, I should confirm my status as an AI and mention that I'm here to assist. Then, I can offer assistance with various topics. I should keep the tone friendly and open-ended to encourage further conversation.\n",
      "\n",
      "I should avoid any technical jargon and keep the response simple. Let me check for any possible misunderstandings. No, the user just wants to know my current state and ability to help. Alright, that should cover it.\n",
      "</think>\n",
      "content: Hello! I'm here to help with anything you need. How can I assist you today? üòä\n",
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you, today?\" I need to respond appropriately. Let me think about the best way to answer.\n",
      "\n",
      "First, I should acknowledge their question. It's a friendly way to start. Maybe say something like \"Hi! How are you today?\" That's polite and shows interest.\n",
      "\n",
      "Then, I can add a bit more to keep the conversation going. Maybe mention something about the day, like \"I'm feeling great today!\" to show I'm happy. \n",
      "\n",
      "I should keep the tone positive and friendly. Avoid any negative language. Let me make sure the response is concise but warm. Alright, that should work.\n",
      "</think>\n",
      "content: Hi! How are you today? I'm super happy to hear that! I'm feeling great, and I'm excited to be here. Let me know if there's anything you're curious about! üòä\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([52.7083,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:7'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([57.0833,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:7'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[1][0],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba6ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,    872,    198,   4340,    525,    498,     30, 151645,    198,\n",
      "         151644,  77091,    198, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:7'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:7')}\n",
      "tensor([[   198, 151667,    198,  32313,     11,    279,   1196,   4588,     11,\n",
      "            330,   4340,    525,    498,   7521,    358,   1184,    311,   5889,\n",
      "            304,    264,  11657,    323,  10950,   1616,     13,   6771,    752,\n",
      "           1744,    911,    279,   1850,   5486,    382,   5338,     11,    358,\n",
      "           1265,  24645,    862,   3405,     13,   1084,    594,    264,   4285,\n",
      "          42113,     11,    773,   7196,   1191,    448,    330,  13048,   1052,\n",
      "           8958,    311,   1473,    358,   2776,   6247,    311,   1492,     13,\n",
      "           5005,     11,    358,    646,    912,    264,   2699,    803,    911,\n",
      "            847,   1482,   1584,     11,   1075,    330,     40,   2776,   3730,\n",
      "           1632,    323,    358,   2776,   1588,    311,   1492,    498,    448,\n",
      "           4113,   8958,   2938,   6696,    264,   5530,    315,   1660,   1052,\n",
      "            369,   1105,    382,     40,   1265,   1281,   2704,    279,   2033,\n",
      "            374,   5810,    323,    537,   2238,  15908,     13,  10696,    912,\n",
      "            264,  11657,  42365,    311,   2506,    432,   3100,     13,   7281,\n",
      "             11,   1779,    421,    279,   1196,    702,    894,   3151,   4755,\n",
      "            476,   3880,  12994,     11,    714,   2474,    807,   1101,   4588,\n",
      "            330,   4340,    525,    498,   7521,    358,   1265,   2506,    432,\n",
      "           4586,    382,  14190,     11,    374,   1052,   4113,    770,    358,\n",
      "           1265,   2908,     30,  10696,    912,    264,   1795,   5239,   3405,\n",
      "            311,  14907,   4623,  16230,     11,    714,    279,   1196,   3207,\n",
      "            944,   2548,   4113,    770,     13,   2055,     11,  10282,    432,\n",
      "          63594,    323,   6785,   1265,    387,   1661,     13,   6771,    752,\n",
      "           2182,    429,    678,   3786,    624, 151668,    271,  13048,   1052,\n",
      "              0,    358,   2776,   6247,    311,   1492,      0,    358,   2776,\n",
      "           3730,   1632,    323,    358,   2776,   1588,    311,   7789,    498,\n",
      "            448,   4113,    498,   1184,      0,  26525,    232, 151645, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [151667,    198,  32313,     11,    279,   1196,    374,  10161,     11,\n",
      "            330,   4340,    525,    498,     11,   3351,   7521,   2055,    358,\n",
      "           1184,    311,   5889,    304,    264,  11657,    323,  10950,  11566,\n",
      "             13,   6771,    752,   1191,    553,  60608,    862,   3405,     13,\n",
      "            358,   1265,   1977,   2494,   1075,     11,    330,  18665,   1052,\n",
      "              0,   2585,    525,    498,   3351,   7521,   2938,    594,  30339,\n",
      "            323,   4933,    358,   2776,  14289,    382,   5847,     11,    358,\n",
      "           1366,    311,   3010,  12994,     13,  10696,   2548,    421,   1052,\n",
      "            594,   4113,    358,    646,   1492,    448,     13,   2938,    594,\n",
      "           1661,    311,   2506,    279,  10435,   1787,     13,    358,   1265,\n",
      "           1281,   2704,    847,   2033,    374,   6785,    323,   1787,  83075,\n",
      "             11,    773,    807,   2666,  10655,  11560,    803,    382,     40,\n",
      "           1265,   5648,    894,   8225,   4128,    323,   2506,    432,    678,\n",
      "           3100,     13,  10696,    912,    264,  15289,     88,   3579,    476,\n",
      "            264,  11657,  42365,    311,   2506,    279,  16232,   8205,     13,\n",
      "           7281,     11,   1779,    369,    894,  12752,  83789,    311,   5978,\n",
      "            279,   2033,    374,   8311,     13,   8704,    358,   2776,    458,\n",
      "          15235,     11,    358,   1513,    944,    614,   4345,  11449,     11,\n",
      "            773,    358,   1184,    311,   2506,    432,   4586,    382,  10061,\n",
      "            752,   2182,    432,    678,   3786,     25,    330,  18665,   1052,\n",
      "              0,   2585,    525,    498,   3351,     30,    358,   2776,   1588,\n",
      "            311,   1492,      0,  26525,    232,      1,   2938,   1265,   3421,\n",
      "            279,  58786,     13,   7093,  15934,    369,    894,  13580,    966,\n",
      "            476,  68125,  61072,   5975,     13,  21607,     11,    429,   4977,\n",
      "           1661,    624, 151668,    271,  18665,   1052,      0,   2585,    525,\n",
      "            498,   3351,     30,    358,   2776,   1588,    311,   1492,      0,\n",
      "          26525,    232, 151645]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"right\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") \n",
    "print(model_inputs)\n",
    "print(output.sequences[:,model_inputs.input_ids.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e965fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond in a friendly and helpful way. Let me think about the best approach.\n",
      "\n",
      "First, I should acknowledge their question. It's a simple greeting, so maybe start with \"Hi there!\" to show I'm happy to help. Then, I can add a bit more about my current state, like \"I'm doing well and I'm here to help you with anything!\" That gives a sense of being there for them.\n",
      "\n",
      "I should make sure the response is natural and not too formal. Maybe add a friendly emoji to keep it light. Also, check if the user has any specific questions or needs assistance, but since they just asked \"How are you?\" I should keep it general.\n",
      "\n",
      "Wait, is there anything else I should consider? Maybe add a follow-up question to encourage further interaction, but the user didn't ask anything else. So, keeping it concise and positive should be good. Let me put that all together.\n",
      "</think>\n",
      "content: Hi there! I'm happy to help! I'm doing well and I'm here to assist you with anything you need! üòä\n",
      "thinking content: <think>\n",
      "Okay, the user is asking, \"How are you, today?\" So I need to respond in a friendly and helpful manner. Let me start by acknowledging their question. I should say something like, \"Hey there! How are you today?\" That's straightforward and shows I'm listening.\n",
      "\n",
      "Next, I want to offer assistance. Maybe ask if there's anything I can help with. That's good to keep the conversation open. I should make sure my response is positive and open-ended, so they feel comfortable sharing more.\n",
      "\n",
      "I should avoid any negative language and keep it all light. Maybe add a smiley face or a friendly emoji to keep the tone warm. Also, check for any cultural nuances to ensure the response is appropriate. Since I'm an AI, I don't have personal experiences, so I need to keep it general.\n",
      "\n",
      "Let me put it all together: \"Hey there! How are you today? I'm here to help! üòä\" That should cover the essentials. Double-check for any typos or grammatical errors. Yeah, that seems good.\n",
      "</think>\n",
      "content: Hey there! How are you today? I'm here to help! üòä\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "933b2ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([47.2917,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:7'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([52.7083,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:7'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[0][1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
